{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from random import choice\n",
    "import requests \n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_proxies() -> list:\n",
    "    '''\n",
    "    \n",
    "    This function scrapes clarketm proxy-list for proxy addresses.\n",
    "    \n",
    "    The function returns a list of proxy addresses.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #  scraping https://github.com/clarketm/proxy-list/blob/master/proxy-list-raw.txt\n",
    "    proxy_url = 'https://github.com/clarketm/proxy-list/blob/master/proxy-list-raw.txt'\n",
    "    proxy_html = requests.get(proxy_url)\n",
    "    proxy_soup = BeautifulSoup(proxy_html.text, 'lxml')\n",
    "    \n",
    "    unclean_proxies = proxy_soup('td', class_ = 'blob-code blob-code-inner js-file-line')\n",
    "    proxies = [proxy.text.strip() for proxy in unclean_proxies]\n",
    "    \n",
    "    return proxies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_randomproxy() -> dict:\n",
    "    return {'http': choice(crawl_proxies())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_crawler(city=None, start=0, end=0,snooze=10) -> pd.DataFrame:\n",
    "    '''\n",
    "    \n",
    "    This function loops over a specified number of pages of the website page to be scraped.\n",
    "    \n",
    "    The function accepts the base URL of the website,page number to start scraping from, \n",
    "    page number to end scraping at, and the number of seconds to wait in-between scraping.\n",
    "    \n",
    "    The function returns nothing. This is more of a progress display for the scraping process.\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    #  empty list to stare dataframes to be concatenated\n",
    "    scraped_df_list = []\n",
    "    \n",
    "    for page_num in tqdm(range(start, end+10, 10), desc='Scraping pages'):\n",
    "        #  url dictionary for the seven cities that are being scraped\n",
    "        #  just a side note, Indeed's URL is difficult to work with smh \n",
    "        cities_url_dict = {'manila': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=Manila&jlid=2d385e0e7a50644e&sort=date&start={page_num}&vjk=2e7fe3a30378027c',\n",
    "                           'makati': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=Makati&jlid=e42eba2843e635b1&sort=date&start={page_num}&vjk=380b3f4742a8dcb1',\n",
    "                           'taguig': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=Taguig&jlid=3e4c70dafd758056&sort=date&start={page_num}&vjk=a367e6c4c1cb7b8b',\n",
    "                           'quezon': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=Quezon%20City&jlid=fb06069655d21c80&sort=date&start={page_num}&vjk=4179e8c746ff8463',\n",
    "                           'pasig': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=Pasig&jlid=b249f65c60df5fc2&sort=date&start={page_num}&vjk=112e13939fc4dc04',\n",
    "                           'mandaluyong': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=Mandaluyong&jlid=d1f10b8ea5946746&sort=date&start={page_num}&vjk=46535244932a7347',\n",
    "                           'san juan': f'https://ph.indeed.com/jobs?q&l=National%20Capital%20Region&rbl=San%20Juan&jlid=aa25f5461d6d7365&sort=date&start={page_num}&vjk=24760d0b2c8fbbdc'} \n",
    "        \n",
    "        #  appending specific page number to the base url\n",
    "        #  but first, we have to identify which link to use\n",
    "        if city == 'manila':\n",
    "            specific_url = cities_url_dict['manila']\n",
    "        elif city == 'makati':\n",
    "            specific_url = cities_url_dict['makati']\n",
    "        elif city == 'taguig':\n",
    "            specific_url = cities_url_dict['taguig']\n",
    "        elif city == 'quezon':\n",
    "            specific_url = cities_url_dict['quezon']\n",
    "        elif city == 'pasig':\n",
    "            specific_url = cities_url_dict['pasig']\n",
    "        elif city == 'mandaluyong':\n",
    "            specific_url = cities_url_dict['mandaluyong']\n",
    "        elif city == 'san juan':\n",
    "            specific_url = cities_url_dict['san juan']\n",
    "        else:\n",
    "            print('Please specify a city')\n",
    "            break\n",
    "        \n",
    "        #  retrieving the html of the specific url\n",
    "        specific_html = requests.get(specific_url, proxies=get_randomproxy())\n",
    "        \n",
    "        #  status code of page: to determine if the page is accessible\n",
    "        status_code = specific_html.status_code\n",
    "        \n",
    "        #  indicating that the process will start \n",
    "        if status_code != 200:\n",
    "            print(f'Retrieval of {specific_url} failed with status code {status_code}')\n",
    "            break\n",
    "        print(f'Retrieving: {specific_url} --- Success!')\n",
    "        \n",
    "        #  function that retrieves data from the specific html and appends it to empty lists\n",
    "        scraped_df_list.append(get_joblisting(specific_html))\n",
    "        \n",
    "        #  giving the spider a little break\n",
    "        time.sleep(snooze)\n",
    "    \n",
    "    return concat_dataframelist(scraped_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joblisting(specific_html=None) -> pd.DataFrame: \n",
    "    '''\n",
    "    \n",
    "    This function scrapes the lamudi.com.ph website for properties.\n",
    "    \n",
    "    The function accepts the retrieved html of a page.\n",
    "    \n",
    "    The function returns a dataframe containing the scraped data.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #  empty lists to store data retrieved from page\n",
    "    listing_list = []\n",
    "    location_list = []\n",
    "    salary_list = []\n",
    "    company_list = []\n",
    "    \n",
    "    #  list of list to pass to dataframe_fromlists()\n",
    "    features_list = [listing_list,\n",
    "                     location_list,\n",
    "                     salary_list,\n",
    "                     company_list,\n",
    "                     ]\n",
    "    \n",
    "    #  soupify the html passed\n",
    "    joblist_soup = BeautifulSoup(specific_html.text, 'lxml')\n",
    "    \n",
    "    #  data of each proprety is concentrated in this html div tag with the class: ListingCell-AllInfo ListingUnit\n",
    "    jobinfo_group = joblist_soup.find_all('td', class_ = 'resultContent')\n",
    "    \n",
    "    #  looping through each property listing from propinfo_group\n",
    "    for job in jobinfo_group:\n",
    "        \n",
    "        #  retrieve listing name \n",
    "        listing = job.find('span').next_element.next_element.text.strip()\n",
    "        \n",
    "        #  retrieve location \n",
    "        location = job.find('div', class_ = 'companyLocation').text.strip()\n",
    "        \n",
    "        #  retrieve salary\n",
    "        try:  \n",
    "            salary = job.find('div', class_ = 'salary-snippet').next_element.text.strip()           \n",
    "        except AttributeError:\n",
    "            salary = np.nan\n",
    "        \n",
    "        #  retrieve company\n",
    "        company = job.find('span', class_ = 'companyName').text.strip()\n",
    "\n",
    "        #  appending data to empty lists \n",
    "        listing_list.append(listing)\n",
    "        location_list.append(location)\n",
    "        salary_list.append(salary)\n",
    "        company_list.append(company)\n",
    "\n",
    "\n",
    "    #  returning a dataframe\n",
    "    return dataframe_fromlists(features_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_fromlists(features_list) -> pd.DataFrame:\n",
    "    '''\n",
    "    \n",
    "    This function creates a dataframe from the data lists.\n",
    "\n",
    "    The function accepts a list of lists containing the data to be converted to a dataframe.\n",
    "    \n",
    "    The function returns a dataframe.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_format = {'listing': features_list[0], \n",
    "                'location': features_list[1], \n",
    "                'salary': features_list[2], \n",
    "                'company': features_list[3], \n",
    "                }\n",
    "    \n",
    "    #  creating a dataframe from the data lists\n",
    "    df = pd.DataFrame(data = df_format)\n",
    "    \n",
    "    #  returning the dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dataframelist(scraped_df_list) -> pd.DataFrame:\n",
    "    '''\n",
    "    \n",
    "    This function concatenates a list of dataframes.\n",
    "\n",
    "    The function accepts a list of dataframes to be concatenated.\n",
    "    \n",
    "    The function returns a dataframe.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #  establishing first dataframe point\n",
    "    prime_df = scraped_df_list[0]\n",
    "    \n",
    "    #  looping through remaining dataframes to concatenate\n",
    "    for i in range(len(scraped_df_list)-1):\n",
    "        prime_df = pd.concat([prime_df,scraped_df_list[i+1]], ignore_index=True)\n",
    "    \n",
    "    return prime_df "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45f1c52e6ff181b6a08f617934a1e0b0b4cce9123eb7ba2c516144485a5a3822"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('data-science-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
